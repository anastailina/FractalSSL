{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ec7499",
   "metadata": {},
   "source": [
    "# PAMAP2 ➜ TS2Vec ➜ Fractal‑SSL  \n",
    "An interactive, step‑by‑step Jupyter workflow.\n",
    "\n",
    "**Sections**\n",
    "1. Inspect & preprocess PAMAP2\n",
    "2. Pre‑train a TS2Vec backbone\n",
    "3. Enhance with Fractal‑SSL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269bd9f",
   "metadata": {},
   "source": [
    "## 0  Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d766d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ⚠️ Run once per environment\n",
    "%pip install -q torch==2.3.0 ts2vec pandas scikit-learn tqdm hydra-core\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56c277",
   "metadata": {},
   "source": [
    "## 1  Imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22012f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json, math, random, itertools, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ts2vec import TS2Vec\n",
    "\n",
    "RAW_ROOT   = Path('/Users/ai421/UK Dementia Research Institute Dropbox/Anastasia Ilina/FractalSSL/data/pamap2+physical+activity+monitoring/PAMAP2_Dataset').expanduser()   # <- edit if needed\n",
    "CACHE_ROOT = Path('./cache')\n",
    "CACHE_ROOT.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe45f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ai421/UK Dementia Research Institute Dropbox/Anastasia Ilina/FractalSSL/data/pamap2+physical+activity+monitoring/PAMAP2_Dataset')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "RAW_ROOT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349382b9",
   "metadata": {},
   "source": [
    "### Helper to read a single PAMAP2 `.dat` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aefcc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = [\n",
    "    \"timestamp\", \"activity_id\", \"heart_rate\",\n",
    "    # IMU columns...\n",
    "]  # shortened for brevity – full list in spec\n",
    "\n",
    "def load_dat(path: Path, cols=COLS, downsample_hr=True):\n",
    "    \"\"\"Returns a DataFrame (100 Hz) with NaNs handled.\"\"\"\n",
    "    df = pd.read_csv(path, sep=' ', header=None)\n",
    "    df.columns = cols + [f'col_{i}' for i in range(len(df.columns)-len(cols))]\n",
    "\n",
    "    # Optional: down‑sample HR (original 9 Hz) – simple forward‑fill\n",
    "    if downsample_hr:\n",
    "        df['heart_rate'].replace(-1, np.nan, inplace=True)\n",
    "        df['heart_rate'].interpolate(method='linear', limit_direction='both', inplace=True)\n",
    "\n",
    "    # Normalise sensor channels (z‑score per column)\n",
    "    sensor_cols = df.columns.drop(['timestamp', 'activity_id'])\n",
    "    df[sensor_cols] = (df[sensor_cols] - df[sensor_cols].mean()) / df[sensor_cols].std()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1594c7ca",
   "metadata": {},
   "source": [
    "### Load & peek at Subject 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578c9d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 .dat files I see:\n",
      " • Protocol/subject108.dat\n",
      " • Protocol/subject109.dat\n",
      " • Protocol/subject107.dat\n",
      " • Protocol/subject106.dat\n",
      " • Protocol/subject104.dat\n"
     ]
    }
   ],
   "source": [
    "print('First 5 .dat files I see:')\n",
    "for p in list(RAW_ROOT.rglob('*.dat'))[:5]:\n",
    "    print(' •', p.relative_to(RAW_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e886d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/mdcnqz1d7673k0zpfm275hrm0000gp/T/ipykernel_11901/981279570.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['heart_rate'].replace(-1, np.nan, inplace=True)\n",
      "/var/folders/nb/mdcnqz1d7673k0zpfm275hrm0000gp/T/ipykernel_11901/981279570.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['heart_rate'].interpolate(method='linear', limit_direction='both', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>...</th>\n",
       "      <th>col_41</th>\n",
       "      <th>col_42</th>\n",
       "      <th>col_43</th>\n",
       "      <th>col_44</th>\n",
       "      <th>col_45</th>\n",
       "      <th>col_46</th>\n",
       "      <th>col_47</th>\n",
       "      <th>col_48</th>\n",
       "      <th>col_49</th>\n",
       "      <th>col_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.38</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.839332</td>\n",
       "      <td>-1.965332</td>\n",
       "      <td>0.898811</td>\n",
       "      <td>0.446885</td>\n",
       "      <td>0.030369</td>\n",
       "      <td>0.895952</td>\n",
       "      <td>0.471171</td>\n",
       "      <td>-0.064588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008444</td>\n",
       "      <td>0.014729</td>\n",
       "      <td>-0.017610</td>\n",
       "      <td>-0.401004</td>\n",
       "      <td>-1.138542</td>\n",
       "      <td>-2.322759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.39</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.839332</td>\n",
       "      <td>-1.965332</td>\n",
       "      <td>0.869927</td>\n",
       "      <td>0.440123</td>\n",
       "      <td>0.075092</td>\n",
       "      <td>0.888978</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.020714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006492</td>\n",
       "      <td>-0.008282</td>\n",
       "      <td>-0.007561</td>\n",
       "      <td>-0.348879</td>\n",
       "      <td>-1.136321</td>\n",
       "      <td>-2.335681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.40</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.839332</td>\n",
       "      <td>-1.965332</td>\n",
       "      <td>0.899021</td>\n",
       "      <td>0.446949</td>\n",
       "      <td>0.041749</td>\n",
       "      <td>0.874937</td>\n",
       "      <td>0.428003</td>\n",
       "      <td>0.047642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>-0.000352</td>\n",
       "      <td>0.004828</td>\n",
       "      <td>-0.383407</td>\n",
       "      <td>-1.153368</td>\n",
       "      <td>-2.331433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.41</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.839332</td>\n",
       "      <td>-1.965332</td>\n",
       "      <td>0.852074</td>\n",
       "      <td>0.432989</td>\n",
       "      <td>0.074625</td>\n",
       "      <td>0.879650</td>\n",
       "      <td>0.427961</td>\n",
       "      <td>0.047633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>-0.034234</td>\n",
       "      <td>-0.001454</td>\n",
       "      <td>-0.370695</td>\n",
       "      <td>-1.132127</td>\n",
       "      <td>-2.331288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.42</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.839332</td>\n",
       "      <td>-1.965332</td>\n",
       "      <td>0.876366</td>\n",
       "      <td>0.491239</td>\n",
       "      <td>0.086386</td>\n",
       "      <td>0.863274</td>\n",
       "      <td>0.439690</td>\n",
       "      <td>0.056590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012859</td>\n",
       "      <td>-0.024295</td>\n",
       "      <td>-0.009347</td>\n",
       "      <td>-0.413875</td>\n",
       "      <td>-1.152522</td>\n",
       "      <td>-2.327013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  activity_id  heart_rate     col_0     col_1     col_2     col_3  \\\n",
       "0       8.38            0   -0.839332 -1.965332  0.898811  0.446885  0.030369   \n",
       "1       8.39            0   -0.839332 -1.965332  0.869927  0.440123  0.075092   \n",
       "2       8.40            0   -0.839332 -1.965332  0.899021  0.446949  0.041749   \n",
       "3       8.41            0   -0.839332 -1.965332  0.852074  0.432989  0.074625   \n",
       "4       8.42            0   -0.839332 -1.965332  0.876366  0.491239  0.086386   \n",
       "\n",
       "      col_4     col_5     col_6  ...    col_41    col_42    col_43    col_44  \\\n",
       "0  0.895952  0.471171 -0.064588  ...  0.008444  0.014729 -0.017610 -0.401004   \n",
       "1  0.888978  0.430758  0.020714  ... -0.006492 -0.008282 -0.007561 -0.348879   \n",
       "2  0.874937  0.428003  0.047642  ...  0.003137 -0.000352  0.004828 -0.383407   \n",
       "3  0.879650  0.427961  0.047633  ...  0.003298 -0.034234 -0.001454 -0.370695   \n",
       "4  0.863274  0.439690  0.056590  ...  0.012859 -0.024295 -0.009347 -0.413875   \n",
       "\n",
       "     col_45    col_46  col_47  col_48  col_49  col_50  \n",
       "0 -1.138542 -2.322759     NaN     NaN     NaN     NaN  \n",
       "1 -1.136321 -2.335681     NaN     NaN     NaN     NaN  \n",
       "2 -1.153368 -2.331433     NaN     NaN     NaN     NaN  \n",
       "3 -1.132127 -2.331288     NaN     NaN     NaN     NaN  \n",
       "4 -1.152522 -2.327013     NaN     NaN     NaN     NaN  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_path = next(RAW_ROOT.glob('*Protocol/subject101.dat'))\n",
    "df101 = load_dat(sample_path)\n",
    "df101.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4abd713",
   "metadata": {},
   "source": [
    "### Cache all subjects (optional, ~1 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aade97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 14/14 [00:00<00:00, 11607.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for path in tqdm(list(RAW_ROOT.rglob('*.dat')), desc='Caching'):\n",
    "    subj = path.stem.split('.')[0]\n",
    "    out = CACHE_ROOT / f'{subj}.pkl'\n",
    "    if out.exists(): \n",
    "        continue\n",
    "    load_dat(path).to_pickle(out)\n",
    "print('Done ✅')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d69f1",
   "metadata": {},
   "source": [
    "## 2  TS2Vec pre‑training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d12ce852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: ts2vec 0.1\n",
      "Uninstalling ts2vec-0.1:\n",
      "  Successfully uninstalled ts2vec-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/yuezhihan/ts2vec.git@main\n",
      "  Cloning https://github.com/yuezhihan/ts2vec.git (to revision main) to /private/var/folders/nb/mdcnqz1d7673k0zpfm275hrm0000gp/T/pip-req-build-nmw08e6r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/yuezhihan/ts2vec.git /private/var/folders/nb/mdcnqz1d7673k0zpfm275hrm0000gp/T/pip-req-build-nmw08e6r\n",
      "  Resolved https://github.com/yuezhihan/ts2vec.git to commit b0088e14a99706c05451316dc6db8d3da9351163\n",
      "\u001b[31mERROR: git+https://github.com/yuezhihan/ts2vec.git@main does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y ts2vec\n",
    "%pip install git+https://github.com/yuezhihan/ts2vec.git@main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0445eb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ts2vec\n",
      "  Using cached ts2vec-0.1-py3-none-any.whl.metadata (53 bytes)\n",
      "Using cached ts2vec-0.1-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: ts2vec\n",
      "Successfully installed ts2vec-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ts2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ccd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8eafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_iter(data, win_sec=5, stride_sec=2, fs=100):\n",
    "    win = win_sec * fs\n",
    "    stride = stride_sec * fs\n",
    "    for start in range(0, len(data) - win, stride):\n",
    "        yield data[start:start+win].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "960c3e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows: 14344\n"
     ]
    }
   ],
   "source": [
    "class PAMAP2Windows(torch.utils.data.Dataset):\n",
    "    def __init__(self, cache_root=CACHE_ROOT):\n",
    "        self.paths = list(cache_root.glob('subject*.pkl'))\n",
    "        self.windows = []\n",
    "        for p in self.paths:\n",
    "            # DON’T call .to_numpy() here\n",
    "            df = pd.read_pickle(p).drop(columns=['timestamp', 'activity_id'])\n",
    "            self.windows += list(window_iter(df))\n",
    "    def __len__(self): return len(self.windows)\n",
    "    def __getitem__(self, idx): \n",
    "        return torch.from_numpy(self.windows[idx]).float()\n",
    "\n",
    "\n",
    "dataset = PAMAP2Windows()\n",
    "print('Total windows:', len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e765443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked: (14344, 500, 52) ≈ 1.49 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, gc, torch\n",
    "\n",
    "# ── Build a single 3-D array (may take ~1-2 GB of RAM)\n",
    "all_windows = np.stack(dataset.windows, axis=0).astype(np.float32)\n",
    "print('Stacked:', all_windows.shape, f'≈ {all_windows.nbytes/1e9:.2f} GB')\n",
    "\n",
    "# free the Python list; we only need the big array from here on\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5f7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.mps.is_available()\n",
    "# → True  ✅\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37e3bd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c43a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0: loss=743.1087387800217\n",
      "Epoch #1: loss=193.11450375829423\n",
      "Epoch #2: loss=33.75749032838004\n",
      "Epoch #3: loss=32981.79550596646\n",
      "Epoch #4: loss=15748.238873038974\n"
     ]
    }
   ],
   "source": [
    "enc = TS2Vec(\n",
    "    input_dims=52,\n",
    "    output_dims=320,\n",
    "    device=device,\n",
    "    batch_size=256,   # hyper-params live in the constructor\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "enc.fit(all_windows, n_epochs=20, verbose=True)\n",
    "enc.save('ts2vec_backbone.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5120d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project a random batch and inspect shape\n",
    "batch = torch.from_numpy(all_windows[np.random.choice(len(all_windows), 32)]).float().to(device)\n",
    "embeddings = enc(batch)\n",
    "print('Batch shape:', batch.shape, '→ Embeddings shape:', embeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb01d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, pandas as pd, torch, random\n",
    "from pathlib import Path\n",
    "\n",
    "class PAMAP2LabeledWindows(Dataset):\n",
    "    def __init__(self, raw_root, win_sec=5, stride_sec=2, fs=100):\n",
    "        self.X, self.y = [], []\n",
    "        win, stride = win_sec*fs, stride_sec*fs\n",
    "        for f in Path(raw_root).rglob('subject*.dat'):\n",
    "            df = load_dat(f)                         # reuse helper\n",
    "            data = df.drop(columns=['timestamp']).to_numpy(dtype=np.float32)\n",
    "            acts = df['activity_id'].to_numpy()\n",
    "            for start in range(0, len(df)-win, stride):\n",
    "                end = start+win\n",
    "                self.X.append(data[start:end, 1:])   # sensor channels\n",
    "                # majority label in the window:\n",
    "                label = np.bincount(acts[start:end]).argmax()\n",
    "                self.y.append(label)\n",
    "        self.X, self.y = np.stack(self.X), np.array(self.y, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):  return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]), self.y[idx]\n",
    "\n",
    "labeled_ds = PAMAP2LabeledWindows(RAW_ROOT)\n",
    "print('Windows:', len(labeled_ds), 'Shape:', labeled_ds[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168cebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.eval()                                     # freeze gradients\n",
    "with torch.no_grad():\n",
    "    feats = enc.encode(\n",
    "        torch.from_numpy(labeled_ds.X).to(device)   # (N, L, C)\n",
    "    )\n",
    "# TS2Vec returns (N, L, D); take the *last* timestep:\n",
    "feats = feats[:, -1, :].cpu().numpy()              # (N, D)\n",
    "labels = labeled_ds.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    feats, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=500, multi_class='multinomial', solver='lbfgs')\n",
    "clf.fit(Xtr, ytr)\n",
    "\n",
    "pred = clf.predict(Xte)\n",
    "print(classification_report(yte, pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(yte, pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('PAMAP2 – TS2Vec linear probe')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18463cc",
   "metadata": {},
   "source": [
    "## 3  Fractal‑SSL fine‑tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalViewGenerator:\n",
    "    \"\"\"Produces recursively smaller crops of the same window.\"\"\"\n",
    "    def __init__(self, levels=4):\n",
    "        self.levels = levels\n",
    "    def __call__(self, ts):\n",
    "        L = ts.shape[0]\n",
    "        views = []\n",
    "        for i in range(self.levels):\n",
    "            frac = 1 / (2 ** i)\n",
    "            win = int(L * frac)\n",
    "            start = random.randint(0, L - win)\n",
    "            views.append(ts[start:start+win])\n",
    "        return views\n",
    "gen = FractalViewGenerator()\n",
    "print([v.shape[0] for v in gen(batch[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbc5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalSSL(torch.nn.Module):\n",
    "    def __init__(self, backbone, proj_dim=128, levels=4, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.backbone     = backbone          # frozen or fine-tuned\n",
    "        self.levels       = levels\n",
    "        self.temperature  = temperature\n",
    "\n",
    "        self.proj = torch.nn.Sequential(\n",
    "            torch.nn.Linear(backbone.repr_dims, proj_dim),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(proj_dim, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, ts_batch):              # ts_batch: (B, L, C) on device\n",
    "        # --------------------------------- generate multi-scale views\n",
    "        view_lists = [FractalViewGenerator(self.levels)(ts) for ts in ts_batch]\n",
    "        flat_views = [v.float() for sub in view_lists for v in sub]   # all on same device\n",
    "        lengths    = [v.shape[0] for v in flat_views]\n",
    "\n",
    "        # pad to (N, max_len, C)  ────────────────────────────────────\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(flat_views, batch_first=True)\n",
    "        mask   = torch.arange(padded.size(1), device=padded.device)[None, :] < torch.tensor(lengths, device=padded.device)[:, None]\n",
    "\n",
    "        # encode → (N, T, D)  ----------------------------------------\n",
    "        emb = self.backbone.encode(padded)                       # TS2Vec handles mask internally if you pass it; else ignore masked timesteps\n",
    "        emb = emb.mean(dim=1)                                    # simple temporal pooling\n",
    "\n",
    "        proj = self.proj(emb)                                    # (N, proj_dim)\n",
    "        proj = torch.nn.functional.normalize(proj, dim=-1)       # cosine similarity works better\n",
    "\n",
    "        return proj, view_lists                                  # keep view grouping for the loss\n",
    "\n",
    "\n",
    "model = FractalSSL(enc).to(device)               # backbone already on device\n",
    "opt    = torch.optim.AdamW(model.proj.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "BATCH  = 64\n",
    "for step in range(100):\n",
    "    idx = random.sample(range(len(all_windows)), BATCH)\n",
    "    ts  = torch.stack([torch.from_numpy(all_windows[i]) for i in idx]).to(device)  # (B, L, C)\n",
    "\n",
    "    proj, views = model(ts)                    # proj: (B*levels, D)\n",
    "\n",
    "    # -------- build InfoNCE targets --------------------------------\n",
    "    n_views  = len(views[0])                   # levels\n",
    "    targets  = torch.arange(BATCH, device=device).repeat_interleave(n_views)\n",
    "    # targets[i] = original sample id\n",
    "\n",
    "    logits   = torch.matmul(proj, proj.T) / model.temperature          # (N,N)\n",
    "    loss     = torch.nn.CrossEntropyLoss()(logits, targets)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f'{step:>3} | loss = {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ed404",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'fractal_ts2vec.pt')\n",
    "print('Saved to fractal_ts2vec.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1) embed\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    emb = model.backbone.encode(torch.from_numpy(all_windows).to(device))\n",
    "    emb = emb[:, -1, :].cpu().numpy()\n",
    "\n",
    "# 2) train/test split + linear probe\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(emb, labels, stratify=labels, test_size=0.2, random_state=0)\n",
    "clf = LogisticRegression(max_iter=500, solver='lbfgs', multi_class='multinomial').fit(X_tr, y_tr)\n",
    "\n",
    "print(classification_report(y_te, clf.predict(X_te), digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67a9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ledd_calculation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
